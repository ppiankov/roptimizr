# üß† roptimizr.sh ‚Äî Kubernetes Resource Optimizer

roptimizr scans all Kubernetes pods (excluding system namespaces) and identifies containers that are:
	‚Ä¢	CPU-hot
	‚Ä¢	Memory-heavy
	‚Ä¢	Restarting
	‚Ä¢	CrashLooping
	‚Ä¢	OOMKilled

It then prints human-readable recommendations for updated CPU/memory requests & limits, plus a summary of cluster resource usage.

# Why roptimizr?

If you're dealing with Kubernetes performance issues, CPU throttling,
CrashLoopBackOff, or oversized resource limits, roptimizr can help by
automatically analyzing cluster metrics and generating safe, optimized
recommendations based on real usage.

Works with kubectl, metrics-server, and supports restart detection,
CPU-hot logic, limit/request inspection, and cluster capacity summary.

## üìå Usage

```bash
chmod +x roptimizr.sh
./roptimizr.sh
export KUBECONFIG=/path/to/config
```

Helps DevOps engineers identify pods with incorrectly configured
resource limits/requests, reducing cluster waste and improving stability.

# ‚ö° OOMKill Detection & Behavior

LLM workloads, JVM services, Python apps with sudden heap bursts, and anything with malloc-spikes often get OOMKilled before metrics-server ever sees the peak usage.

That means:

Observed usage is always lower than the real peak.

To avoid deceptive metrics, roptimizr.sh follows this rule:

## üî• If a container was OOMKilled:
	‚Ä¢	Ignore observed memory usage (it‚Äôs fake)
	‚Ä¢	Double the existing memory limit
	‚Ä¢	Ensure at least +256Mi bump
	‚Ä¢	Set memory request to 70% of the new limit

Example:
| Situation | Old Mem Limit | New Mem Limit |
|-------|-----|-----------|
| Light web app | 256Mi | 512Mi|
| JVM app | 512Mi | 1024Mi |
| LLM inferencer | 2Gi | 4G |


# üöÄ Aggressive Mode (for LLM workloads)

LLM-serving pods (vLLM, Text-Generation-Inference, Ollama, Triton, etc.) tend to use short bursts of RAM 2‚Äì4√ó higher than stable operation.

Enable aggressive mode:
```bash
./roptimizr.sh --aggressive
```

This changes OOMKilled behavior to:
	‚Ä¢	Triple memory limit (instead of doubling)
	‚Ä¢	Guarantee at least +1Gi bump
	‚Ä¢	Requests set to 80% of the limit**

This mode is ideal for:
	‚Ä¢	LLM text generation
	‚Ä¢	Embeddings batched inference
	‚Ä¢	Vector DB internal memory maps
	‚Ä¢	FastAPI + model in RAM workloads


# üß™ OOMKill Scenarios Detected

## Scenario 1: Silent LLM RAM spike

Symptoms:
	‚Ä¢	Observed usage: 500Mi
	‚Ä¢	Limit: 1024Mi
	‚Ä¢	Actual spike: 2200Mi (never captured by metrics)
	‚Ä¢	Pod OOMKilled instantly

Your output:
```bash
Reason:
  ‚Ä¢ Container suffered OOMKills ‚Üí usage metrics unreliable
  ‚Ä¢ Applied safety rule: doubled memory limit, increased request
```

## Scenario 2: JVM service warming up
	‚Ä¢	Stable usage: 200Mi
	‚Ä¢	Limit: 256Mi
	‚Ä¢	OOMKill during GC or heap expansion

New recommended limit: 512Mi

## Scenario 3: Bursty Python API
	‚Ä¢	Uses Pydantic, llama.cpp bindings, transformers, or large model loads
	‚Ä¢	Occasional burst allocations kill the pod

# üì¶ Summary Output

At the end of a run you get cluster planning metrics:
	‚Ä¢	current total CPU/memory
	‚Ä¢	projected totals after fixes
	‚Ä¢	cluster allocatable capacity
	‚Ä¢	pods with no limits set

Example:
```bash
Current total requested CPU:  2200m
After suggested changes, req: 2600m

Cluster allocatable CPU:      8000m
```

# üõ∞Ô∏è Node Affinity Hotspot Detection

Besides CPU/memory optimization, roptimizr.sh now analyzes how pods are distributed across nodes and identifies situations where workloads are unintentionally ‚Äúover-pinned‚Äù through nodeAffinity.

## Why this matters

Hard-pinning many pods to the same node can cause:
	‚Ä¢	uneven node load
	‚Ä¢	scheduling failures
	‚Ä¢	long pending queues
	‚Ä¢	resource hotspots
	‚Ä¢	unpredictable autoscaling behavior

In other words: you accidentally built a tiny dictatorship where all pods must live on the same node. This helps you notice when that‚Äôs happening.

## How it works

During a scan, the script:
	1.	Tracks how many pods run on each node
	2.	Counts how many of them have explicit nodeAffinity rules
	3.	Flags nodes where:
	‚Ä¢	‚â• 5 pods are using nodeAffinity and
	‚Ä¢	‚â• 70% of all pods on that node are affinity-pinned

This produces an output like:

```bash
============= NODE AFFINITY CHECK =============
‚ö†Ô∏è  Node: worker-llm-01
    ‚Ä¢ Pods on node:      14
    ‚Ä¢ With nodeAffinity: 12 (85%)
    ‚Ä¢ Hint: A large share of workloads here are hard-pinned via nodeAffinity.
      Consider relaxing affinity / adding anti-affinity or spreading across more nodes.
===============================================
```

If nothing suspicious is detected:

```bash
No obvious nodeAffinity hotspots detected.
```

This helps DevOps engineers detect subtle cluster imbalance and affinity misconfigurations before they cause outages or weird scheduling behavior.

## ‚ú® Keywords

kubernetes resource optimization
kubectl top limits requests
automatic resource rightsizing
pod resource analyzer
bash kubernetes script
autoscaling troubleshooting
crashloopbackoff analysis


---

